---
title: "BDA - Project Work"
author: "Jacopo Losi, Nicola Saljoughi"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '1'
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '1'
---
```{r setup, include=FALSE}
# This chunk just sets echo = TRUE as default (i.e. print all code)
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE)

library(aaltobda)
library(arules)
library(brms)
library(devtools)
library(dplyr)
library(easyGgplot2)
library(epitools)
library(ggplot2)
library(KernSmooth)
library(loo)
library(magrittr)
library(MASS)
library(mvtnorm)
library(nnet)
library(phonTools)
library(rcompanion)
library(rstan)
library(rstanarm)
library(tableone)
library(tinytex)

```

```{r}

datFile <- "suicide attempt data_2.csv"
datCsv <- read.csv(datFile, stringsAsFactors=FALSE)
datSet <- as.data.frame(datCsv)

datSet$Season <- datSet$Month
datSet$Month = NULL

## Remove unknown labels

indexUnkn_1 <- which(datSet$Education == 'unknown')
indexUnkn_2 <- which(datSet$Urban == 'unknown')
indexUnkn_3 <- which(datSet$Occupation == 'others/unknown')
datSet <- datSet[-c(indexUnkn_1, indexUnkn_2,indexUnkn_3),]


# Hospitalised
indexHosp   <- which(datSet$Hospitalised == 'yes')
indexNoHosp <- which(datSet$Hospitalised == 'no')
datSet$Hospitalised[indexHosp]   <- 1    # 1 --> yes
datSet$Hospitalised[indexNoHosp] <- 0    # 0 --> no


# Died
indexDied   <- which(datSet$Died == 'yes')
indexNoDied <- which(datSet$Died == 'no')
datSet$Died[indexDied]   <- 1    # 1 --> yes
datSet$Died[indexNoDied] <- 0    # 0 --> no


# Urban
indexUrban   <- which(datSet$Urban == 'yes')
indexNoUrban <- which(datSet$Urban == 'no')
datSet$Urban[indexUrban]   <- 1    # 1 --> yes
datSet$Urban[indexNoUrban] <- 0    # 0 --> no

#Year
indexYear2009 <- which(datSet$Year == 2009)
indexYear2010 <- which(datSet$Year == 2010)
indexYear2011 <- which(datSet$Year == 2011)
datSet$Year[indexYear2009] <- 1    # 1 --> 2009
datSet$Year[indexYear2010] <- 2    # 2 --> 2010
datSet$Year[indexYear2011] <- 3    # 3 --> 2011

# Sex
indexMale   <- which(datSet$Sex == 'male')
indexFemale <- which(datSet$Sex == 'female')
datSet$Sex[indexMale]   <- 1    # 1 --> male
datSet$Sex[indexFemale] <- 0    # 0 --> female

# Education
indexEduZero  <- which(datSet$Education == 'iliterate') 
indexEduOne   <- which(datSet$Education == 'primary') 
indexEduTwo   <- which(datSet$Education == 'Secondary')
indexEduThree <- which(datSet$Education == 'Tertiary')

datSet$Education[indexEduZero]   <- 0   # 0 --> iliterate
datSet$Education[indexEduOne]    <- 1   # 1 --> primary
datSet$Education[indexEduTwo]    <- 2   # 2 --> Secondary
datSet$Education[indexEduThree]  <- 3   # 3 --> Tertiary


# Occupation
indexUnEmpl <- which(datSet$Occupation == 'unemployed')
indexFarm   <- which(datSet$Occupation == 'farming')
indexProf   <- which(datSet$Occupation == 'business/service' | datSet$Occupation == 'professional' | datSet$Occupation == 'worker')

datSet$Occupation[indexUnEmpl]                            <- 0   # 0 --> unemployed                          
datSet$Occupation[indexFarm]                              <- 1   # 1 --> farming
datSet$Occupation[indexProf]                              <- 2   # 2 --> professional and worker
datSet$Occupation[-c(indexUnEmpl, indexFarm, indexProf)]  <- 3   # 3 --> others

# Method 
indexPesticide <- which(datSet$method == 'Pesticide')
indexPoison    <- which(datSet$method == 'Other poison')
indexHanging   <- which(datSet$method == 'hanging')
indexOthers    <- which(datSet$method != 'Pesticide' &
                        datSet$method != 'Other poison' &
                        datSet$method != 'hanging')

datSet$method[indexPesticide] <- 1 # 1 --> Pesticide
datSet$method[indexPoison]    <- 2 # 2 --> Other poison  
datSet$method[indexHanging]   <- 3 # 3 --> hanging
datSet$method[indexOthers]    <- 4 # 4 --> All others

# Season
indexSpring <- which(datSet$Season >= 3 & datSet$Season <= 5)
indexSummer <- which(datSet$Season >= 6 & datSet$Season <= 8)
indexAutumn <- which(datSet$Season >= 9 & datSet$Season <= 11)
indexWinter <- which(datSet$Season == 12 | datSet$Season <= 2)

datSet$Season[indexSpring] <- 1  # 1 --> Spring
datSet$Season[indexSummer] <- 2  # 2 --> Summer
datSet$Season[indexAutumn] <- 3  # 3 --> Autumn
datSet$Season[indexWinter] <- 4  # 4 --> Winter

```


```{r echo=FALSE}
## DATA => MYDATA
# In this chunk we convert the data entries to make it 
# treatable in the following analysis

mydata <- data
mydata$Season <- data$Month
mydata$Month = NULL


# Hospitalised

indexHosp   <- which(data$Hospitalised == 'yes')
indexNoHosp <- which(data$Hospitalised == 'no')

mydata$Hospitalised[indexHosp]   <- 1    # 1 --> yes
mydata$Hospitalised[indexNoHosp] <- 0    # 0 --> no


# Died

indexDied   <- which(data$Died == 'yes')
indexNoDied <- which(data$Died == 'no')

mydata$Died[indexDied]   <- 1    # 1 --> yes
mydata$Died[indexNoDied] <- 0    # 0 --> no


# Urban

indexUrban   <- which(data$Urban == 'yes')
indexNoUrban <- which(data$Urban == 'no')

mydata$Urban[indexUrban]   <- 1    # 1 --> yes
mydata$Urban[indexNoUrban] <- 0    # 0 --> no

#Year
indexYear2009 <- which(mydata$Year == 2009)
indexYear2010 <- which(mydata$Year == 2010)
indexYear2011 <- which(mydata$Year == 2011)

mydata$Year[indexYear2009] <- 1    # 1 --> 2009
mydata$Year[indexYear2010] <- 2    # 2 --> 2010
mydata$Year[indexYear2011] <- 3    # 3 --> 2011


# Season

indexSpring <- which(data$Month >= 3 & data$Month <= 5)
indexSummer <- which(data$Month >= 6 & data$Month <= 8)
indexAutumn <- which(data$Month >= 9 & data$Month <= 11)
indexWinter <- which(data$Month == 12 | data$Month <= 2)

mydata$Season[indexSpring] <- 1  # 1 --> Spring
mydata$Season[indexSummer] <- 2  # 2 --> Summer
mydata$Season[indexAutumn] <- 3  # 3 --> Autumn
mydata$Season[indexWinter] <- 4  # 4 --> Winter


# Sex

indexMale   <- which(data$Sex == 'male')
indexFemale <- which(data$Sex == 'female')

mydata$Sex[indexMale]   <- 1    # 1 --> male
mydata$Sex[indexFemale] <- 0    # 0 --> female


# Age

indexAgeOne   <- which(data$Age <= 34) 
indexAgeTwo   <- which(data$Age >= 35 & data$Age <= 49)
indexAgeThree <- which(data$Age >= 50 & data$Age <= 64)
indexAgeFour  <- which(data$Age >= 65)

mydata$Age[indexAgeOne]   <- 1   # 1 --> <34
mydata$Age[indexAgeTwo]   <- 2   # 2 --> 35-49
mydata$Age[indexAgeThree] <- 3   # 3 --> 50-64
mydata$Age[indexAgeFour]  <- 4   # 4 --> >65


# Education

indexEduZero  <- which(data$Education == 'iliterate') 
indexEduOne   <- which(data$Education == 'primary') 
indexEduTwo   <- which(data$Education == 'Secondary')
indexEduThree <- which(data$Education == 'Tertiary')

mydata$Education[indexEduZero]   <- 0   # 0 --> iliterate
mydata$Education[indexEduOne]    <- 1   # 1 --> primary
mydata$Education[indexEduTwo]    <- 2   # 2 --> Secondary
mydata$Education[indexEduThree]  <- 3   # 3 --> Tertiary


# Occupation

indexFarm   <- which(data$Occupation == 'farming')
indexNoFarm <- which(data$Occupation != 'farming')

mydata$Occupation[indexFarm]   <- 1    # 1 --> farming
mydata$Occupation[indexNoFarm] <- 0    # 0 --> non farming


# Method 

indexPesticide <- which(data$method == 'Pesticide')
indexPoison    <- which(data$method == 'Other poison')
indexHanging    <- which(data$method == 'hanging')
indexOthers    <- which(data$method != 'Pesticide' &
                        data$method != 'Other poison' &
                        data$method != 'hanging')

mydata$method[indexPesticide] <- 1 # 1 --> Pesticide
mydata$method[indexPoison]    <- 2 # 2 --> Other poison  
mydata$method[indexHanging]   <- 3 # 3 --> hanging
mydata$method[indexOthers]    <- 4 # 4 --> All others


## Remove unknown labels

indexUnknw1 <- which(mydata$Education == 'unknown')
mydata <- mydata[-indexUnknw1,]
indexUnkn <- which(mydata$Urban == 'unknown')
mydata <- mydata[-indexUnkn,]

```




\clearpage




# Introduction
This project is based on a study carried out in 2015 by a group of researchers to estimate the incidence of serious suicide attempts in Shandong, China, and to examine the factors associated with fatality among the attempters. \newline
We have chosen to examine a dataset on suicides because it is a really important but often underconsidered problem in today's society. Not only this problem reflects a larger problem in a country societal system but it can also be a burden for hospital resources. We think that by being able to talk about it more openly and by truly trying to estimate its size and impact we can start to understand where the causes are rooted and what can be done to fight it. \newline
We invite the reader to check the source section to further read about the setting and results of the named paper. \newline
In this report we will carry out our analysis following the bayesian approach. Since also the frequentist approach was covered during lecture, we though that it was meaningful to compare the two at the begininning of the analysis.\newline
Adopting then the bayesian approach we will first develop a multiple logistic regression model using all the variables, after that we will do variable selection to determine which are the most influential factors and develop a second multiple logistic regression model using the selected variables. After that, we will assess the convergence and efficiency of the models, do posterior predictive checking and compare the models. To conclude we carry out sensitivity analysis with respect to prior choices and eventually answer our analysis problem. 


## Analysis Problem

The objective of the project is to use the bayesian approach to develop models to evaluate the most influential factors related to serious suicide attempts (SSAs, defined as suicide attempts resulting in either death or hospitalisation) and being able to make predictions for the years following the period where the study was set. 



## Data 
Data from two independent health surveillance systems were linked, constituted by records of suicide deaths and hospitalisations that occured among residents in selected countries during 2009-2011.  
The data set is constituted by 2571 observations of 11 variables:
\begin{itemize}
  \item \texttt{Person\_ID}: ID number, $1,...,2571$
  \item \texttt{Hospitalised}: \textit{yes} or \textit{no}
  \item \texttt{Died}: \textit{yes} or \textit{no}
  \item \texttt{Urban}: \textit{yes}, \textit{no} or \textit{unknown}
  \item \texttt{Year}: $2009$, $2010$ or $2011$
  \item \texttt{Month}: $1,...,12$
  \item \texttt{Sex}: \textit{female} or \textit{male}
  \item \texttt{Age}: years
  \item \texttt{Education}: \textit{iliterate}, \textit{primary}, \textit{Secondary}, \textit{Tertiary} or \textit{unknown}
  \item \texttt{Occupation}: one of ten categories
  \item \texttt{method}: one of nine methods
\end{itemize}
It is important to notice that the population in the study is predominantly rural and that the limitation of the study is that the incidence estimates are likely to be underestimated due to underreporting in both surveillance systems. 




## Source

Sun J, Guo X, Zhang J, Wang M, Jia C, Xu A (2015) "Incidence and fatality of serious suicide attempts in a predominantly rural population in Shandong, China: a public health surveillance study," BMJ Open 5(2): e006762. https://doi.org/10.1136/bmjopen-2014-006762

Data downloaded via Dryad Digital Repository. https://doi.org/10.5061/dryad.r0v35




\clearpage




# Analysis
In this section we will carry out our analysis.




## Model description 
In order to evaluate the factors which influence the probability of SSA the most it is an obvious chioice to develop a multiple logistic regression model.
Two different models have been implemented which will then be compared in the following analysis:
\begin{itemize}
  \item \textbf{Full logistic regression model} where all the parameters are included
  \item \textbf{Reduced logistic regression model} that only includes the parameters selected in the variable selection phase. 
\end{itemize}




### Full Logistic Regression Model





### Reduced Logistic Regression Model




## Prior choices



## Code 

### Data loading 

First of all we load the data. Notice that some processing was done on the original data removing samples with missing entries (that resulted to constitute less than the 6 % of the dataset) and turning labels from strings into integers. 
```{r}

## Create Stan data
dat <- list(N        = nrow(mydata),
            p        = ncol(mydata) - 2,
            died     = as.numeric(mydata$Died),
            urban    = as.numeric(mydata$Urban),
            year     = as.numeric(mydata$Year),
            season   = as.numeric(mydata$Season),
            sex      = as.numeric(mydata$Sex),
            age      = as.numeric(mydata$Age),
            edu      = as.numeric(mydata$Education),
            job      = as.numeric(mydata$Occupation),
            method   = as.numeric(mydata$method))

```

In this phase we are working on testing different models, therefore it is worth to take only some random samples from the data. As a matter of fact, the dataset that we have is big and thus the computation on the whole dataset will take a lot of time.

Therefore, we will proceed as follows:
* we will generate a vector of 50 random number taken from our dataset;
* we will test the models with this data, that are sufficient for not loosing in generality;
* we will run the final model on the whole dataset.

```{r}

random_index <- sample(mydata$Person_ID, size = 50, replace = TRUE)

data_reduced <- mydata[random_index, ]
data_reduced <- na.omit(data_reduced)
```

```{r}

## Create Stan data
dat_red <- list(N    = nrow(data_reduced),
                p        = ncol(data_reduced) - 2,
                died     = as.numeric(data_reduced$Died),
                urban    = as.numeric(data_reduced$Urban),
                year     = as.numeric(data_reduced$Year),
                season   = as.numeric(data_reduced$Season),
                sex      = as.numeric(data_reduced$Sex),
                age      = as.numeric(data_reduced$Age),
                edu      = as.numeric(data_reduced$Education),
                job      = as.numeric(data_reduced$Occupation),
                method   = as.numeric(data_reduced$method))

```


### Full logistic regression model
Here we start by implementing the full logistic regression model.  

```{r}
## FULL LOGISTIC REGRESSION MODEL

## Load Stan Model
fileNameOne <- "./logistic_regression_model.stan"
stan_code_full <- readChar(fileNameOne, file.info(fileNameOne)$size)
cat(stan_code_full)

```

### Stan Code Running 
The Stan models are run by using five chains constituted by 2000 iterations, a warmup length of 800 iterations and a thin equal to 10. Thin is a positive integer that specifies the period for saving samples; it is set by default = 1, and it is normally left to defaults. In our case though our posterior distribution takes up a lot of memory even when using a reduced dataset and we require a large numer of iteration to achieve effective sample size and therefore we decide to set it to 10 in this phase. 

```{r echo=FALSE}
## SIMPLE LOGISTIC REGRESSION MODEL

# Run Stan
resStanFull <- stan(model_code = stan_code_full,
                data = dat_red,
                chains = 5, 
                iter = 2000, 
                warmup = 800,
                thin = 10,
                refresh = 0,
                seed = 12345,
                control = list(adapt_delta = 0.95))
print(resStanFull, pars = c('beta'))

```


### Variable selection 
In this section we will evaluate the most influential factors and their correlation in order to select the most descriptive ones that will be used to contruct our second model (the reduced logistic regression model).\newline
First of all we process our data:
```{r}
# Transform fitting over beta in a dataframe for the plots

beta_matrix <- zeros(length(extract(resStanFull)$beta[,1]), ncol(data_reduced) - 2)

for (i in 1:ncol(data_reduced) - 2)
  beta_matrix[,i] = beta_matrix[,i] + extract(resStanFull)$beta[,i]

beta_df <- as.data.frame(beta_matrix)
```

Now we show traceplots and generate scatter plots in order to evaluate the correlation between the parameters:

```{r}
# Show traceplot
traceplot(resStanFull, pars = c('beta[3]','beta[4]', 'beta[5]', 
                                'beta[6]', 'beta[7]', 'beta[8]',
                                'beta[9]'), inc_warmup = TRUE)
# Generate some scatter plots in order to see the correlations between parameters
scatter_1 <- ggplot(beta_df, aes(x=V3, y=V7)) +
                    ggtitle("Correlation between location and education") +
                    xlab("Urban") + ylab("Education") +
             geom_point(size=1, shape=23) +
             geom_smooth(method=lm, linetype="dashed", color="darkred", fill="blue")

scatter_2 <- ggplot(beta_df, aes(x=V3, y=V8)) +
                    ggtitle("Correlation between location and occuption") +
                    xlab("Urban") + ylab("Occupation") +
             geom_point(size=1, shape=23) +
             geom_smooth(method=lm, linetype="dashed", color="darkred", fill="blue")

scatter_3 <- ggplot(beta_df, aes(x=V5, y=V6)) +
                    ggtitle("Correlation between gender and age") +
                    xlab("Gender") + ylab("Age") +
             geom_point(size=1, shape=23) +
             geom_smooth(method=lm, linetype="dashed", color="darkred", fill="blue")


ggplot2.multiplot(scatter_1,scatter_2,scatter_3, cols=1)
```
Now we overlay histogram, density and mean value of the parameters. The most interesting plots are presented; using the mean value is interesting since we can understand which weakly informative priors can be designed. 
```{r}

plot_1 <- qplot(extract(resStanFull)$beta[,3], geom = 'blank',
                xlab = 'Values of weigth', ylab = 'Occurences', main='Urbans') +   
  geom_histogram(aes(y = ..density..),col = I('red'), bins = 50) + 
  geom_line(aes(y = ..density..), size = 1, col = I('blue'), stat = 'density', ) +
  geom_vline(aes(xintercept=mean(extract(resStanFull)$beta[,3])), col=I('yellow'), linetype="dashed", size=1) 

plot_2 <- qplot(extract(resStanFull)$beta[,5], geom = 'blank',
                xlab = 'Values of weigth', ylab = 'Occurences', main='Sex') +   
  geom_histogram(aes(y = ..density..),col = I('red'), bins = 50) + 
  geom_line(aes(y = ..density..), size = 1, col = I('blue'), stat = 'density', ) +
  geom_vline(aes(xintercept=mean(extract(resStanFull)$beta[,5])), col=I('yellow'), linetype="dashed", size=1)

plot_3 <- qplot(extract(resStanFull)$beta[,6], geom = 'blank',
                xlab = 'Values of weigth', ylab = 'Occurences', main='Age') +   
  geom_histogram(aes(y = ..density..),col = I('red'), bins = 50) + 
  geom_line(aes(y = ..density..), size = 1, col = I('blue'), stat = 'density', ) +
  geom_vline(aes(xintercept=mean(extract(resStanFull)$beta[,6])), col=I('yellow'), linetype="dashed", size=1)

plot_4 <- qplot(extract(resStanFull)$beta[,7], geom = 'blank',
                xlab = 'Values of weigth', ylab = 'Occurences', main='Education') +   
  geom_histogram(aes(y = ..density..),col = I('red'), bins = 50) + 
  geom_line(aes(y = ..density..), size = 1, col = I('blue'), stat = 'density', ) +
  geom_vline(aes(xintercept=mean(extract(resStanFull)$beta[,7])), col=I('yellow'), linetype="dashed", size=1)

plot_5 <- qplot(extract(resStanFull)$beta[,8], geom = 'blank',
                xlab = 'Values of weigth', ylab = 'Occurences', main='Occupation') +   
  geom_histogram(aes(y = ..density..),col = I('red'), bins = 50) + 
  geom_line(aes(y = ..density..), size = 1, col = I('blue'), stat = 'density', ) +
  geom_vline(aes(xintercept=mean(extract(resStanFull)$beta[,8])), col=I('yellow'), linetype="dashed", size=1) 

ggplot2.multiplot(plot_1,plot_2,plot_3,plot_4, plot_5, cols=3)

```

From the analysis done above, and especially looking at the histogram, it is clear that the most important parameters that count in our analysis are: the fact that the people come from urban or rural areas, then their education, occupation and partially if they are man or woman. As a matter of fact, the mean and the maximum values of the coeffcient related to those paramters have the bigger magnitude. This means that those parameters are weighted more in the multi regression function in the model.

Therefore, for further analysis, it will be good to develop specific analysis using only these parameters, in order to have a more precise evalution considering only the most relevant parameters. 




### Frequentist approach 

```{r}

outcomeModel <- glm(as.numeric(Died) ~ as.numeric(Urban) + 
                                       as.numeric(Year) + 
                                       as.numeric(Season) + 
                                       as.numeric(Sex) + 
                                       as.numeric(Age) + 
                                       as.numeric(Education) + 
                                       as.numeric(Occupation) + 
                                       as.numeric(method), data = mydata,
                    family = binomial(link = "logit"))
summary(outcomeModel)

```




### Comparison

```{r}
## Bayesian
print(resStanFull, pars = c("beta"))

## Frequentist
tableone::ShowRegTable(outcomeModel, exp = FALSE)

```



### Some clustering on the data

Let us try to cluster the data using the specific year in order to do a prediction on the following year

```{r}

indexYear2009 <- which(mydata$Year == 2009)
data_year_2009 <- mydata[indexYear2009,]

indexYear2010 <- which(mydata$Year == 2010)
data_year_2010 <- mydata[indexYear2010,]

indexYear2011 <- which(mydata$Year == 2011)
data_year_2011 <- mydata[indexYear2011,]
```




## Full logistic regression model 
For completeness we report here the full model once again. 
```{r}
## FULL LOGISTIC REGRESSION MODEL

## Load Stan Model
fileNameOne <- "./logistic_regression_model.stan"
stan_code_full <- readChar(fileNameOne, file.info(fileNameOne)$size)
cat(stan_code_full)

```


## Reduced logistic regression model
We can now implement the reduced logistic regression model using the selected parameters. 

```{r}
## REDUCED LOGISTIC REGRESSION MODEL

## Load Stan Model
fileNameOneDef <- "./logistic_regression_model_def.stan"
stan_code_simple_def <- readChar(fileNameOneDef, file.info(fileNameOneDef)$size)
cat(stan_code_simple_def)

```




## Stan Code Running
Now we are going to run the model on the full dataset. \newline
First we define the Stan data to run the second (reduced) model. 
```{r}

## Create Stan data
dat_def <- list(N        = nrow(mydata),
                p        = 4,
                died     = as.numeric(mydata$Died),
                sex      = as.numeric(mydata$Sex),
                age      = as.numeric(mydata$Age),
                edu      = as.numeric(mydata$Education))

```
We first run the full model. The settings are the same as before except that now we are using the full dataset and default value for thin. 
```{r echo=FALSE}
## FULL LOGISTIC REGRESSION MODEL

# Run Stan
resStanFull <- stan(model_code = stan_code_full,
                data = dat,
                chains = 5, 
                iter = 2000, 
                warmup = 800,
                thin = 1,
                refresh = 0,
                seed = 12345,
                control = list(adapt_delta = 0.95))
print(resStanFull, pars = c('beta'))

```

Now we run the reduced order model..
```{r echo=FALSE}
## REDUCED LOGISTIC REGRESSION MODEL

# Run Stan
resStanRed <- stan(model_code = stan_code_simple_def,
                  data = dat_def,
                  chains = 5, 
                  iter = 2000, 
                  warmup = 800,
                  thin = 1,
                  refresh = 0,
                  seed = 12345,
                  control = list(adapt_delta = 0.95))
print(resStanRed, pars = c('beta'))

```

## Convergence Analysis
In this section we are going to analyse the implemented models, both in terms of convergence (assessed using R-hat and HMC specific convergence diagnostic) and efficiency (by computing the Effective Sample Size). \newline


### R-hat
R-hat convergence diagnostic compares between- and within-chain estimates for model parameters and other univariate quantities of interest. If chains have not mixed well R-hat is larger than 1. In practical terms, it is good practice to use at least four chains and using the sample if R-hat is less than 1.05. \newline
We can see from the result of \texttt{print(fit)} we have just displayed that all the Rhat values are equal to one for both the models and therefore we have convergence. 


### HMC
Here we compute convergence diagnostic specific to Hamiltonian Monte Carlo, and in particular divergences and tree depth.\newline
The following code computes the diagnostic for the full model:
```{r, fig.width=8, fig.height=5, warning=FALSE}
## Full model HMC diagnostic

check_hmc_diagnostics(resStanFull)
```
As we can see none of the interations ended with a divergence nor saturated the maximum tree depth. \newline
Now we compute the diagnostic for the reduced model:
```{r, fig.width=8, fig.height=5, warning=FALSE}
## Reduced model HMC diagnostic

check_hmc_diagnostics(resStanRed)
```
Also for the reduced model none of the iterations ended with a divergence nor saturated the maximum tree depth. 

### ESS
Effective sample size (ESS) measures the amount by which autocorrelation within the chains increases uncertainty in estimates. \newline
As for the Rhat values we can directly observe the effective sample size values of the chains using the command \texttt{print(fit)}, already used. We can see that the sample size values are all sufficiently high for both model. 



## Posterior Predictive Checking 


## Model Comparison




# qplot(datSet$Age, geom = 'blank', xlab = 'Values of weigth', ylab = 'Occurences', main='Urbans') +   
#   geom_histogram(aes(y = ..density..),col = I('red'), bins = 50) + 
#   geom_line(aes(y = ..density..), size = 1, col = I('blue'), stat = 'density', ) +
#   geom_vline(aes(xintercept=mean(datSet$Age)), col=I('yellow'), linetype="dashed", size=1) 
```


```{r}

data_4_fit <- list(N = nrow(datSet),
                   p = 6,
                   age = as.numeric(datSet$Age),
                   died = as.numeric(datSet$Died),
                   sex = as.numeric(datSet$Sex),
                   job = as.numeric(datSet$Occupation),
                   urban = as.numeric(datSet$Urban),
                   edu = as.numeric(datSet$Education))


fileName <- "./stan_model_with_prior.stan"
stan_code <- readChar(fileName, file.info(fileName)$size)
cat(stan_code)

# Run Stan
fitStan <- stan(model_code = stan_code,
                data = data_4_fit,
                chains = 5, 
                iter = 2000, 
                warmup = 800,
                thin = 10,
                refresh = 0,
                seed = 12345,
                control = list(adapt_delta = 0.95))
print(fitStan, pars = c('beta', 'sigma'))


```

```{r}

log_like_model <- extract_log_lik(fitStan, merge_chains = FALSE)
r_eff <- relative_eff(exp(log_like_model))
loo_mod <- loo(log_like_model, r_eff = r_eff)
print(loo_mod)
plot(loo_mod)
elpd_loo_mod <- loo_mod$estimates[1]
elpd_loo_mod

```


```{r}

data_4_fit_complete <- list(N = nrow(datSet),
                   p = 10,
                   age = as.numeric(datSet$Age),
                   died = as.numeric(datSet$Died),
                   hosp = as.numeric(datSet$Hospitalised),
                   year = as.numeric(datSet$Year),
                   sex = as.numeric(datSet$Sex),
                   job = as.numeric(datSet$Occupation),
                   urban = as.numeric(datSet$Urban),
                   edu = as.numeric(datSet$Education),
                   method = as.numeric(datSet$method),
                   season = as.numeric(datSet$Season))


fileName <- "./stan_model_prior_all_params.stan"
stan_code_complete <- readChar(fileName, file.info(fileName)$size)
cat(stan_code_complete)

# Run Stan
fitStan_complete <- stan(model_code = stan_code_complete,
                data = data_4_fit_complete,
                chains = 5, 
                iter = 2000, 
                warmup = 800,
                thin = 10,
                refresh = 0,
                seed = 12345,
                control = list(adapt_delta = 0.95))
print(fitStan_complete, pars = c('beta', 'sigma'))


```


```{r}

log_like_model_compl <- extract_log_lik(fitStan_complete, merge_chains = FALSE)
r_eff <- relative_eff(exp(log_like_model_compl))
loo_mod_comp <- loo(log_like_model_compl, r_eff = r_eff)
print(loo_mod_comp)
plot(loo_mod_comp)
elpd_loo_mod_comp <- loo_mod_comp$estimates[1]
elpd_loo_mod_comp

```



## Age prediction



\clearpage



# Conclusions


## Problems encountered


## Potential improvements



